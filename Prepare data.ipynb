{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Download\n",
    "filename = Directory +\"/\"+ filename+\".jpg\" #อันนี้รองรับการสร้าง Folder แล้วยัดไปเรื่อยๆ \n",
    "ระวังเรื่องการใช้งาน Directory นะ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "# Note to Kagglers: This script will not run directly in Kaggle kernels. You\n",
    "# need to download it and run it on your local machine.\n",
    "\n",
    "# Downloads images from the Google Landmarks dataset using multiple threads.\n",
    "# Images that already exist will not be downloaded again, so the script can\n",
    "# resume a partially completed download. All images will be saved in the JPG\n",
    "# format with 90% compression quality.\n",
    "\n",
    "import sys, os, multiprocessing, csv\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "import os\n",
    "\n",
    "def ParseData(data_file):\n",
    "    csvfile = open(data_file, 'r')\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    key_url_list = [line[:3] for line in csvreader]\n",
    "    return key_url_list[1:]  # Chop off header\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "def download_image(url,filename,Directory):\n",
    "#*************************************** ระวังผิด directory นะ set ก่อนด้วย ****************************\n",
    "        filename = Directory +\"/\"+ filename+\".jpg\" #อันนี้รองรับการสร้าง Folder แล้วยัดไปเรื่อยๆ\n",
    "#*************************************** ระวังผิด directory นะ set ก่อนด้วย ****************************\n",
    "        if os.path.exists(filename):\n",
    "            print('Image %s already exists. Skipping download.' % filename)\n",
    "            return\n",
    "        try:\n",
    "            try :\n",
    "                response = urlopen(url)\n",
    "                image_data = response.read()\n",
    "            except KeyboardInterrupt:\n",
    "                return\n",
    "            except : \n",
    "                print(\"404 NOT FOUND\")\n",
    "        except KeyboardInterrupt:\n",
    "            return\n",
    "        except:\n",
    "            print('Warning: Could not download image %s from %s' % (key, url))\n",
    "            return\n",
    "        try:\n",
    "            pil_image = Image.open(BytesIO(image_data))\n",
    "        except KeyboardInterrupt:\n",
    "            return\n",
    "        except:\n",
    "            print('Warning: Failed to parse image %s' % key)\n",
    "            return\n",
    "        urllib.request.urlretrieve(url,filename)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create downloadable file\n",
    "fomat data เป็น Csv. 3 colurm จะเป็นผมให้มันเป็น แรก 3 line ในคำสั่ง ParseData(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_download = ParseData('landmark_recognition-master/data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['192931', 'york_river_state_park']\n"
     ]
    }
   ],
   "source": [
    "#Play safe for index_boundary tranform list to dict \n",
    "label_to_category = ParseData('landmark_recognition-master/data/label_to_category.csv')\n",
    "label_to_category_dict = {}\n",
    "\n",
    "for el_lable in  label_to_category :\n",
    "    label_to_category_dict[el_lable[2]] = [el_lable[0],el_lable[1]]\n",
    "print(label_to_category_dict[\"0\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91618"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "len(csv_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Split data \n",
    "#ตัวของข้อมูลนั้นทุกตัวคือคนละ Class \n",
    "#Expect ต้องแยกได้ 7 Folder\n",
    "\n",
    "lst_raw_order = [] #L คือ List ของภาพที่เป็นคนละ Class\n",
    "for i in range(len(csv_download)) :  # Loop นี้จะแยกและหาตำแหน่ง สุดท้าย ของ Class แต่ละ Class\n",
    "    if i == len(csv_download)-1 :\n",
    "        lst_raw_order.append(int(csv_download[i][0]))\n",
    "        break\n",
    "    if (csv_download[i][2] != csv_download[i+1][2]) :\n",
    "        #print(i)\n",
    "        lst_raw_order.append(int(csv_download[i][0]))\n",
    "c = 0\n",
    "# for j in lst_data : \n",
    "#     print(c,j,\"\\n\")\n",
    "#     c += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "raw_data = np.asarray(csv_download)\n",
    "lst_raw_data_split = []\n",
    "for i in range(100) : \n",
    "    if ( i == 0 ) : \n",
    "        lst_raw_data_split.append(raw_data[:lst_raw_order[i]])\n",
    "    else : \n",
    "        lst_raw_data_split.append(raw_data[lst_raw_order[i-1]:lst_raw_order[i]] )\n",
    "print(len(lst_raw_data_split))\n",
    "#test Miss data in list\n",
    "for j in range(100) : \n",
    "    for el_lst in range(1,len(lst_raw_data_split[i])) : \n",
    "        if (lst_raw_data_split[i][el_lst][2] != lst_raw_data_split[i][el_lst][2] ) : \n",
    "            print(\"error_detect\",lst_raw_data_split[1][el_lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "test_set  = []\n",
    "var_set   = []\n",
    "Data = []\n",
    "for el_class in lst_raw_data_split :\n",
    "    el_class = pd.DataFrame(el_class)\n",
    "    train, validate, test = np.split(el_class.sample(frac=1), [int(.8*len(el_class)), int(.9*len(el_class))])\n",
    "    train_set.append(train)\n",
    "    test_set.append(test)\n",
    "    var_set.append(validate)\n",
    "    Data.append(list([train,test,validate]))\n",
    "#from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(100) :\n",
    "    for i in range(3) :\n",
    "        temp_dir = j \n",
    "        directory = label_to_category_dict[str(temp_dir)][1]\n",
    "        if ( i == 0  ) : \n",
    "            base_directory = \"D:/Train\"\n",
    "            if not (os.path.exists(base_directory)) :\n",
    "                os.makedirs(base_directory)\n",
    "            pd.DataFrame(train_set[j]).to_csv(\"D:/Train/\"+str(j)+\".csv\")\n",
    "        elif (i == 1 ) : \n",
    "            base_directory = \"D:/Test\"\n",
    "            if not (os.path.exists(base_directory)) :\n",
    "                os.makedirs(base_directory)\n",
    "            pd.DataFrame(test_set[j]).to_csv( \"D:/Test/\"+str(j)+\".csv\")\n",
    "        elif (i == 2 ) : \n",
    "            base_directory = \"D:/Validate\"\n",
    "            if not (os.path.exists(base_directory)) :\n",
    "                os.makedirs(base_directory)\n",
    "            pd.DataFrame(var_set[j]).to_csv(\"D:/Validate/\"+str(j)+\".csv\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Download\n",
    "\n",
    "แปลงเป็น Function ที่สามารถเรียกค่า ของ Directory และจัดการไฟล์"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': ['192931', 'york_river_state_park'],\n",
       " '1': ['186080', 'osaka_prefectural_flower_garden'],\n",
       " '2': ['19605', 'saiful_muluk_lake'],\n",
       " '3': ['171683', 'lok_virsa_museum,_islamabad'],\n",
       " '4': ['189811', 'lake_como'],\n",
       " '5': ['86869', 'mount_rainier_national_park'],\n",
       " '6': ['90021', 'rohtas_fort'],\n",
       " '7': ['62798', 'enchanted_floral_gardens_of_kula'],\n",
       " '8': ['181631', 'leesylvania_state_park'],\n",
       " '9': ['10618', 'botanical_garden_jevremovac'],\n",
       " '10': ['165596', 'edinburgh_castle'],\n",
       " '11': ['165900', 'mount_arapiles'],\n",
       " '12': ['145015', 'texas_state_library_and_archives_commission'],\n",
       " '13': ['171772', 'university_of_chicago_library'],\n",
       " '14': ['67416', 'manassas_national_battlefield_park'],\n",
       " '15': ['177194', 'royal_national_park'],\n",
       " '16': ['107750', 'chippokes_plantation_state_park'],\n",
       " '17': ['137203', 'port_of_casablanca'],\n",
       " '18': ['73300', 'iguazu_falls'],\n",
       " '19': ['149980', 'khotyn_fortress'],\n",
       " '20': ['176018', 'hayravank_monastery'],\n",
       " '21': ['184313', 'kahanu_garden'],\n",
       " '22': ['16658', 'glasnevin_cemetery'],\n",
       " '23': ['182973', 'central_park'],\n",
       " '24': ['83144', 'museum_of_folk_architecture_and_ethnography_in_pyrohiv'],\n",
       " '25': ['60532', 'national_m._m._grishko_botanical_garden_(kiev)'],\n",
       " '26': ['133563', 'occoneechee_state_park'],\n",
       " '27': ['80019', 'botanical_garden_kit'],\n",
       " '28': ['1924', 'niagara_falls'],\n",
       " '29': ['98993', 'lake_bled'],\n",
       " '30': ['120885', 'lake_ohrid'],\n",
       " '31': ['196358', 'luray_caverns'],\n",
       " '32': ['70716', 'yuyuan_gardens'],\n",
       " '33': ['33992', 'grayson_highlands_state_park'],\n",
       " '34': ['11544', 'oleksandria_park'],\n",
       " '35': ['107917', 'botanical_garden_in_kryvyi_rih'],\n",
       " '36': ['84689', 'first_landing_state_park'],\n",
       " '37': ['97734', 'lake_wendouree'],\n",
       " '38': ['41648', 'akkerman_fortress'],\n",
       " '39': ['31094', 'olesko_castle'],\n",
       " '40': ['51699', 'itatiaia_national_park'],\n",
       " '41': ['88596', 'fomin_botanical_garden'],\n",
       " '42': ['179380', 'skole_beskids_national_nature_park'],\n",
       " '43': ['38482', 'skopje_fortress'],\n",
       " '44': ['47378', 'eiffel_tower'],\n",
       " '45': ['20409', 'noraduz_cemetery'],\n",
       " '46': ['10419', 'qutb_minar_and_its_monuments,_delhi'],\n",
       " '47': ['46705', 'grand_canyon'],\n",
       " '48': ['45428', 'sofiyivsky_park'],\n",
       " '49': ['93198', 'high_bridge_trail_state_park'],\n",
       " '50': ['148331', 'west_lake'],\n",
       " '51': ['63529', 'recoleta_cemetery'],\n",
       " '52': ['110398', 'wilderness_road_state_park'],\n",
       " '53': ['168106', 'bryce_canyon_national_park'],\n",
       " '54': ['108327', 'chapada_dos_veadeiros_national_park'],\n",
       " '55': ['144313', 'royal_botanic_garden_edinburgh'],\n",
       " '56': ['187209', 'novodevichy_cemetery_(moscow)'],\n",
       " '57': ['169349', 'beamish_museum'],\n",
       " '58': ['162833', 'pakistan_monument_islamabad'],\n",
       " '59': ['85758', 'caledon_state_park'],\n",
       " '60': ['93154', \"sailor's_creek_battlefield_historical_state_park\"],\n",
       " '61': ['46500', 'pilkington_library'],\n",
       " '62': ['29794', 'bigorski_monastery'],\n",
       " '63': ['139706', 'natural_tunnel_state_park'],\n",
       " '64': ['120734', 'mathura_museum'],\n",
       " '65': ['101399', 'ljubljana_castle'],\n",
       " '66': ['69193', 'lake_burrumbeet'],\n",
       " '67': ['142343', 'false_cape_state_park'],\n",
       " '68': ['65855', 'spring_grove_cemetery,_cincinnati'],\n",
       " '69': ['31361', 'mount_wutai'],\n",
       " '70': ['97717', 'john_d._rockefeller_jr._library'],\n",
       " '71': ['25953', 'fairy_stone_state_park'],\n",
       " '72': ['62074', 'flower_festival_commemorative_park'],\n",
       " '73': ['202388', 'lewis_ginter_botanical_garden'],\n",
       " '74': ['194521', 'aguinaldo_shrine'],\n",
       " '75': ['124923', 'mason_neck_state_park'],\n",
       " '76': ['24852', 'yellowstone_national_park'],\n",
       " '77': ['168098', 'golden_gate_bridge'],\n",
       " '78': ['78197', 'hungry_mother_state_park'],\n",
       " '79': ['41808', 'douthat_state_park'],\n",
       " '80': ['107164', 'st._hripsime_church_in_vagharshapat'],\n",
       " '81': ['6208', 'kardzhali_history_museum'],\n",
       " '82': ['76303', 'saint_holy_mother_church_of_yeghvard'],\n",
       " '83': ['2079', 'brompton_cemetery'],\n",
       " '84': ['199506', 'shrine_mont'],\n",
       " '85': ['60912', 'craters_of_the_moon_national_monument'],\n",
       " '86': ['187779', 'genoese_fortress_(sudak)'],\n",
       " '87': ['25093', 'matka_canyon'],\n",
       " '88': ['5004', 'castle_heeswijk'],\n",
       " '89': ['111311', 'west_norwood_cemetery'],\n",
       " '90': ['62831', 'sky_meadows_state_park'],\n",
       " '91': ['146497', 'lake_constance'],\n",
       " '92': ['5170', 'ellora_caves'],\n",
       " '93': ['201840', 'neghuts_monastery'],\n",
       " '94': ['136302', 'bakhchisaray_palace'],\n",
       " '95': ['109169', 'ogrodzieniec_castle'],\n",
       " '96': ['47712', 'cambridge_university_botanic_garden'],\n",
       " '97': ['49712', 'westmoreland_state_park'],\n",
       " '98': ['127523', 'meadowlark_botanical_gardens'],\n",
       " '99': ['90647', 'tel_aviv_port']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_category_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#*************************************** ระวังผิด directory นะ set ก่อนด้วย ****************************\n",
    " #Use disc directory\n",
    "#*************************************** ระวังผิด directory นะ set ก่อนด้วย ****************************\n",
    "def Download_image(start,end) :   \n",
    "    start_time = time.time()\n",
    "     # label_to_category_dict[str(temp_dir)][1] จะเรียก ชื่อ Landmark จาก Dict มา\n",
    "    for j in range(start,end): \n",
    "        temp_dir = j \n",
    "        directory = label_to_category_dict[str(temp_dir)][1]\n",
    "        print(\"Do CLass 1\")\n",
    "        for i in range(3) :\n",
    "                if ( i == 0  ) : \n",
    "                    base_directory = \"D:/Train\"\n",
    "                    if not (os.path.exists(base_directory)) :\n",
    "                        os.makedirs(base_directory)\n",
    "                elif (i == 1 ) : \n",
    "                    base_directory = \"D:/Test\"\n",
    "                    if not (os.path.exists(base_directory)) :\n",
    "                        os.makedirs(base_directory)\n",
    "                elif (i == 2 ) : \n",
    "                    base_directory = \"D:/Validate\"\n",
    "                    if not (os.path.exists(base_directory)) :\n",
    "                        os.makedirs(base_directory)\n",
    "                    else : \n",
    "                        print(\"error create Test-Train-Valiadate\")\n",
    "                        break\n",
    "                for k in range(len(Data[j][i]))  :\n",
    "                    try :\n",
    "                        if os.path.exists(base_directory +'/'+ directory): #ใส่รูปไปเรื่อยๆ จนกว่าจะไปเจอ Directory \n",
    "                                #print('Image %s already exists. Skipping download.' % base_directory + directory)\n",
    "                            download_image(Data[j][i][1].iloc[k],Data[j][i][0].iloc[k],base_directory+'/'+ directory)\n",
    "                        else : \n",
    "                            os.makedirs(base_directory +'/'+ directory) #เมื่อต้องสร้างไฟล์ใหม่ ต้องมีการใส่รูป\n",
    "                            download_image(Data[j][i][1].iloc[k],Data[j][i][0].iloc[k],base_directory+'/'+ directory)\n",
    "                        if ( k % 150 == 0) : \n",
    "                            print(\"Pass\",str(k))\n",
    "                            end_time = time.time()\n",
    "                            print(end_time-start_time)\n",
    "                            start_time = time.time()\n",
    "\n",
    "                    except KeyboardInterrupt:\n",
    "                        return\n",
    "                    except :\n",
    "                        print(\"error\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Do CLass 1\n",
      "Pass 0\n",
      "0.7169208526611328\n",
      "404 NOT FOUND\n",
      "error\n",
      "Pass 1000\n",
      "975.7639648914337\n",
      "Pass 2000\n",
      "974.8709876537323\n",
      "Pass 0\n",
      "99.1708071231842\n",
      "Pass 0\n",
      "263.3346927165985\n",
      "Batch 1\n",
      "Do CLass 1\n",
      "Pass 0\n",
      "2.1317830085754395\n",
      "404 NOT FOUND\n",
      "error\n",
      "Pass 0\n",
      "806.0212104320526\n",
      "error create Test-Train-Valiadate\n",
      "Batch 1\n",
      "Do CLass 1\n",
      "Pass 0\n",
      "1.0513606071472168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Top\\Anaconda3\\lib\\site-packages\\PIL\\TiffImagePlugin.py:802: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 2. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass 0\n",
      "447.1047897338867\n",
      "404 NOT FOUND\n",
      "error\n",
      "error create Test-Train-Valiadate\n",
      "Batch 1\n",
      "Do CLass 1\n",
      "Pass 0\n",
      "0.5470447540283203\n",
      "Pass 0\n",
      "629.485053062439\n",
      "error create Test-Train-Valiadate\n",
      "Batch 1\n",
      "Do CLass 1\n",
      "Pass 0\n",
      "0.6313931941986084\n",
      "Pass 0\n",
      "841.3619570732117\n",
      "404 NOT FOUND\n",
      "error\n",
      "error create Test-Train-Valiadate\n",
      "Batch 1\n",
      "Do CLass 1\n",
      "Pass 0\n",
      "1.8701415061950684\n"
     ]
    }
   ],
   "source": [
    "for  i in range(99) :\n",
    "    print(\"Batch >>>\",i)\n",
    "    Download_image(i,i+1)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
